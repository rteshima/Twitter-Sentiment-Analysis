{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "info final project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3wsPqol16JLB"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG23orMNsOkm",
        "colab_type": "text"
      },
      "source": [
        "# Manual Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMkTVhw7bNY0",
        "colab_type": "code",
        "outputId": "98ade531-e57b-42b9-b4c0-ad1e66675ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "pd.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UFOs_PHae9AX",
        "colab": {}
      },
      "source": [
        "\"\"\"This segment contains methods for reading the file in from the path as \n",
        "well as splitting training data between senses\"\"\"\n",
        "\n",
        "def read_file(path):\n",
        "  with open(path) as fin:\n",
        "    tweets = []\n",
        "    sentiments = []\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    sw = stopwords.words(\"english\")\n",
        "\n",
        "    for line in fin:\n",
        "      arr = line.lower().strip().split()\n",
        "      for i in range(1, len(arr)):\n",
        "        arr[i] = ps.stem(arr[i])\n",
        "      tweets.append(arr[1:])\n",
        "      sentiments.append(arr[0])\n",
        "    \n",
        "    return tweets, sentiments\n",
        "\n",
        "def split_senses(tweets, sentiments):\n",
        "  positives = []\n",
        "  neutrals = []\n",
        "  negatives = []\n",
        "\n",
        "  for i, tweet in enumerate(tweets):\n",
        "    if sentiments[i] == \"positive\":\n",
        "      positives.append(tweet)\n",
        "    elif sentiments[i] == \"negative\":\n",
        "      negatives.append(tweet)\n",
        "    else:\n",
        "      neutrals.append(tweet)\n",
        "  return positives, neutrals, negatives"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi-2NgaJJ4Uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This next section of code has to do with translating the text data of the \n",
        "tweets into numerical TF-IDF values.\n",
        "\n",
        "The TF-IDF values can then be averaged into centroid values.\"\"\"\n",
        "\n",
        "def calculate_df(tweets):\n",
        "  doc_freqs = defaultdict(int)\n",
        "  for tweet in tweets:\n",
        "    for word in tweet:\n",
        "      doc_freqs[word] += 1\n",
        "  return doc_freqs\n",
        "\n",
        "def calculate_tf(tweets):\n",
        "  all_tfs = []\n",
        "  for tweet in tweets:\n",
        "    cur_tf = defaultdict(float)\n",
        "    term_count = 0\n",
        "    for word in tweet:\n",
        "      cur_tf[word] += 1.0\n",
        "      term_count += 1\n",
        "    \n",
        "    for key in cur_tf.keys():\n",
        "      cur_tf[key] = cur_tf[key] / term_count\n",
        "    \n",
        "    all_tfs.append(cur_tf)\n",
        "  \n",
        "  return all_tfs\n",
        "\n",
        "def calculate_tfidf(doc_freqs, term_freqs):\n",
        "  tfidfs = []\n",
        "  for tweet in term_freqs:\n",
        "    cur_rows_tfidf = defaultdict(float)\n",
        "    for word in tweet.keys():\n",
        "      if word in doc_freqs:\n",
        "        idf = math.log10(len(term_freqs) / (doc_freqs[word] + 1))\n",
        "      else:\n",
        "        idf = math.log10(len(term_freqs) / (1))\n",
        "      cur_tfidf = tweet[word] * idf\n",
        "      cur_rows_tfidf[word] = cur_tfidf\n",
        "    tfidfs.append(cur_rows_tfidf)\n",
        "  return tfidfs\n",
        "\n",
        "def calculate_centroid(tfidfs, doc_freqs):\n",
        "  centroid = defaultdict(float)\n",
        "  for doc in tfidfs:\n",
        "    for word in doc:\n",
        "      centroid[word] += doc[word]\n",
        "    \n",
        "  for word in centroid.keys():\n",
        "    if word in doc_freqs:\n",
        "      centroid[word] = centroid[word] / doc_freqs[word]\n",
        "    else:\n",
        "      centroid[word] = centroid[word] / 1\n",
        "  \n",
        "  return centroid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DF0EbYPViRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This section has methods to compare test data and centroids\n",
        "\"\"\"\n",
        "\n",
        "def cos_similarity(sentiment_centroid, test_document):\n",
        "  sim = 0\n",
        "  numerator = 0\n",
        "  centroid_norm = 0\n",
        "  test_doc_norm = 0\n",
        "  \n",
        "  for term in test_document:\n",
        "    if term in sentiment_centroid:\n",
        "      numerator += (sentiment_centroid[term] * test_document[term])\n",
        "    test_doc_norm += (test_document[term] ** 2)\n",
        "  \n",
        "  for term in sentiment_centroid:\n",
        "    centroid_norm += (sentiment_centroid[term] ** 2)\n",
        "  \n",
        "  test_doc_norm = math.sqrt(float(test_doc_norm))\n",
        "  centroid_norm = math.sqrt(float(centroid_norm))\n",
        "\n",
        "  sim = numerator / (test_doc_norm * centroid_norm)\n",
        "\n",
        "  return sim\n",
        "\n",
        "def compare_similarities(centroids, test_tweet):\n",
        "  pos_sim = cos_similarity(centroids['positive'], test_tweet)\n",
        "  neut_sim = cos_similarity(centroids['neutral'], test_tweet)\n",
        "  neg_sim = cos_similarity(centroids['negative'], test_tweet)\n",
        "\n",
        "  highest_sim = max(pos_sim, neut_sim, neg_sim)\n",
        "\n",
        "  if pos_sim == highest_sim:\n",
        "    return \"positive\"\n",
        "  elif neut_sim == highest_sim:\n",
        "    return \"neutral\"\n",
        "  else:\n",
        "    return \"negative\"\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZhnRg8VdnVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_data(test_path, all_sentiment_centroids):\n",
        "  test_tweets, actual_sentiments = read_file(test_path)\n",
        "\n",
        "  individual_tweet_tfidfs = []\n",
        "  for tweet in test_tweets:\n",
        "    doc_freqs = calculate_df([tweet])\n",
        "    term_freqs = calculate_tf([tweet])\n",
        "    tfidfs = calculate_tfidf(doc_freqs, term_freqs)\n",
        "    # centroid = calculate_centroid(tfidfs, doc_freqs)\n",
        "    individual_tweet_tfidfs.append(tfidfs[0])\n",
        "\n",
        "  experimental_sentiments = []\n",
        "  for tweet_tfidf in individual_tweet_tfidfs:\n",
        "    # print(tweet_tfidf)\n",
        "    result = compare_similarities(all_sentiment_centroids, tweet_tfidf)\n",
        "    experimental_sentiments.append(result)\n",
        "\n",
        "  accuracy = evaluate_accuracy(experimental_sentiments, actual_sentiments)\n",
        "  return accuracy\n",
        "\n",
        "def evaluate_accuracy(experimental_sentiments, actual_sentiments):\n",
        "  num_correct = 0\n",
        "  for i in range(len(experimental_sentiments)):\n",
        "    if experimental_sentiments[i] == actual_sentiments[i]:\n",
        "      num_correct += 1\n",
        "  \n",
        "  percent_correct = float(num_correct) / float(len(experimental_sentiments))\n",
        "  \n",
        "  return percent_correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaJZwkVTCaYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_data(training_path):\n",
        "  #read data in from the txt file\n",
        "  train_tweets, train_sentiments = read_file(training_path)\n",
        "  #split data up from huge tweet list to lists associated with that sense\n",
        "  positives, neutrals, negatives = split_senses(train_tweets, train_sentiments)\n",
        "  \n",
        "  #compute doc frequencies of each sense\n",
        "  pos_dfs = calculate_df(positives)\n",
        "  neut_dfs = calculate_df(neutrals)\n",
        "  neg_dfs = calculate_df(negatives)\n",
        "\n",
        "  #compute term frequency of each sense\n",
        "  pos_tfs = calculate_tf(positives)\n",
        "  neut_tfs = calculate_tf(neutrals)\n",
        "  neg_tfs = calculate_tf(negatives)\n",
        "\n",
        "  # print(str(len(pos_dfs)) + \" should equal \" + str(len(pos_tfs)))\n",
        "  \n",
        "  #compute tfidf of each sense\n",
        "  pos_tfidfs = calculate_tfidf(pos_dfs, pos_tfs)\n",
        "  neut_tfidfs = calculate_tfidf(neut_dfs, neut_tfs)\n",
        "  neg_tfidfs = calculate_tfidf(neg_dfs, neut_tfs)\n",
        "\n",
        "  #compute centroids of each sense\n",
        "  pos_centroid = calculate_centroid(pos_tfidfs, pos_dfs)\n",
        "  neut_centroid = calculate_centroid(neut_tfidfs, neut_dfs)\n",
        "  neg_centroid = calculate_centroid(neg_tfidfs, neg_dfs)\n",
        "\n",
        "  all_centroids = {\n",
        "      \"positive\": pos_centroid,\n",
        "      \"neutral\": neut_centroid,\n",
        "      \"negative\": neg_centroid\n",
        "  }\n",
        "\n",
        "  return all_centroids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1f6ItzaYUPN",
        "colab_type": "code",
        "outputId": "0905a535-7253-49e5-fc29-0605cbc3d783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def main():\n",
        "  data_train = \"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/semeval_train.txt\"\n",
        "  test2013 = '/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2013_raw.txt'\n",
        "  test2014 = '/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2014_raw.txt'\n",
        "  test2015 = '/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2015_raw.txt'\n",
        "  test2016 = '/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2016_raw.txt'\n",
        "\n",
        "  sentiment_centroids = train_data(data_train)\n",
        "  test_results2013 = test_data(test2013, sentiment_centroids)\n",
        "  test_results2014 = test_data(test2014, sentiment_centroids)\n",
        "  test_results2015 = test_data(test2015, sentiment_centroids)\n",
        "  test_results2016 = test_data(test2016, sentiment_centroids)\n",
        "\n",
        "  print(\"2013 accuracy: \" + str(test_results2013))\n",
        "  print(\"2014 accuracy: \" + str(test_results2014))\n",
        "  print(\"2015 accuracy: \" + str(test_results2015))\n",
        "  print(\"2016 accuracy: \" + str(test_results2016))\n",
        "  # for i in range(len(tweets)):\n",
        "  #   print(\"Sentiment: {}    tweet: {}\".format(sentiments[i], tweets[i]))\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2013 accuracy: 0.4176285414480588\n",
            "2014 accuracy: 0.4006479481641469\n",
            "2015 accuracy: 0.42109669317706155\n",
            "2016 accuracy: 0.4486937133439969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ9A065VlMV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wsPqol16JLB",
        "colab_type": "text"
      },
      "source": [
        "# ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYw7Uem57e2a",
        "colab_type": "code",
        "outputId": "f7dfd8cc-63da-4184-ee70-2eb13a4035cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/semeval_train.txt\") as fin:\n",
        "    tweets = []\n",
        "    senses = []\n",
        "    sentiments = []\n",
        "    for line in fin:\n",
        "        arr = line.lower().strip().split()\n",
        "        tweets.append(line)\n",
        "        sentiments.append(arr[0])\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        if sentiments[i] == \"positive\":\n",
        "            senses.append('positive')\n",
        "        elif sentiments[i] == \"negative\":\n",
        "            senses.append('negative')\n",
        "        else:\n",
        "            senses.append('neutral')\n",
        "\n",
        "def features():\n",
        "    vector = TfidfVectorizer()\n",
        "    vector.fit(tweets)\n",
        "    feats = (vector.transform(tweets))\n",
        "    trainX, testX, trainY, testY = train_test_split(feats, senses, train_size = 0.5, test_size = 0.5)\n",
        "    return trainX, testX, trainY, testY\n",
        "\n",
        "def classifier(trainX, testX, trainY, testY):\n",
        "    model = ExtraTreeClassifier()\n",
        "    model = model.fit(trainX, trainY)\n",
        "    prediction = model.predict(testX)\n",
        "    return prediction\n",
        "\n",
        "def main():\n",
        "    trainX, testX, trainY, testY = features()\n",
        "    prediction = classifier(trainX, testX, trainY, testY)\n",
        "    accuracy = accuracy_score(testY, prediction)\n",
        "    print(accuracy)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0.7900976290097629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJGtMM4iNTP6",
        "colab_type": "code",
        "outputId": "28dcd7f1-a858-474b-bde6-4e9652e1733a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "class SentimentClassifier1:\n",
        "    def train(self, trainX, trainY):\n",
        "        self.clf = DummyClassifier(strategy='most_frequent')\n",
        "        self.clf.fit(trainX, trainY)\n",
        "\n",
        "    def classify(self, testX):\n",
        "        # same as return ['EOS' for i in range(len(testX))]\n",
        "        return self.clf.predict(testX)\n",
        "\n",
        "class SentimentClassifier2:\n",
        "    def train(self, trainX, trainY):\n",
        "        self.clf = RandomForestClassifier()\n",
        "        self.clf.fit(trainX, trainY)\n",
        "\n",
        "    def classify(self, testX):\n",
        "        return self.clf.predict(testX)\n",
        "\n",
        "class SentimentClassifier3:\n",
        "    def train(self, trainX, trainY):\n",
        "        self.clf = DecisionTreeClassifier(max_depth=4, max_leaf_nodes=3)\n",
        "        self.clf.fit(trainX, trainY)\n",
        "\n",
        "    def classify(self, testX):\n",
        "        return self.clf.predict(testX)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file) as fin:\n",
        "        X = []\n",
        "        y = []\n",
        "        for line in fin:\n",
        "            #print(line)\n",
        "            arr = line.strip().split('\\t', 1)\n",
        "            X.append(arr[1])\n",
        "            y.append(arr[0])\n",
        "        return X, y\n",
        "\n",
        "def get_sentiments(file):\n",
        "    sentiments = {}\n",
        "    arr = []\n",
        "    with open(file) as fin:\n",
        "         for line in fin:\n",
        "             arr.append(line.split())\n",
        "    for line in arr:\n",
        "        word = line[2][6:]\n",
        "        senti = line[5][14:]\n",
        "        sentiments[word] = senti\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "def addFeatures(xFile):\n",
        "    newFeatures = []\n",
        "    dataset = get_sentiments(\"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/sentiments.txt\")\n",
        "    for i in range(len(xFile)):\n",
        "        newFeatures.append([0,0,0])\n",
        "        tweet = xFile[i]\n",
        "        for word in tweet:\n",
        "            if word.find('!') != -1:\n",
        "                newFeatures[i][0] = 1\n",
        "            if ':)' in word or ';)' in word:\n",
        "                newFeatures[i][1] = 1\n",
        "            elif ':(' in word or ';(' in word:\n",
        "                newFeatures[i][1] = -1\n",
        "            if word in dataset:\n",
        "                if dataset[word] == 'positive':\n",
        "                    newFeatures[i][2] = newFeatures[i][2] + 1\n",
        "                else:\n",
        "                    newFeatures[i][2] = newFeatures[i][2] - 1\n",
        "\n",
        "    return newFeatures\n",
        "\n",
        "def evaluate(outputs, golds):\n",
        "    correct = 0\n",
        "    for h, y in zip(outputs, golds):\n",
        "        if h == y:\n",
        "            correct += 1\n",
        "    print(f'{correct} / {len(golds)}  {correct / len(golds)}')\n",
        "\n",
        "def main():\n",
        "    X, y = load_data(\"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/semeval_train.txt\")\n",
        "    testX, testY = load_data(\"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2016_raw.txt\")\n",
        "\n",
        "\n",
        "    dummy = SentimentClassifier1()\n",
        "    dummy.train(X, y)\n",
        "    outputsDummy = dummy.classify(testX)\n",
        "\n",
        "    newFeatures = addFeatures(X)\n",
        "    newTestFeatures = addFeatures(testX)\n",
        "    randomForest = SentimentClassifier2()\n",
        "    decisionTree = SentimentClassifier3()\n",
        "\n",
        "    randomForest.train(newFeatures, y)\n",
        "    outputsRF = randomForest.classify(newTestFeatures)\n",
        "\n",
        "    decisionTree.train(newFeatures, y)\n",
        "    outputsDecision = decisionTree.classify(newTestFeatures)\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(len(outputsDummy)):\n",
        "        final = []\n",
        "        final.append(outputsDummy[i])\n",
        "        final.append(outputsRF[i])\n",
        "        final.append(outputsDecision[i])\n",
        "        outputs.append(max(set(final), key=final.count))\n",
        "\n",
        "\n",
        "    #print(outputsDummy)\n",
        "    evaluate(outputs, testY)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "11425 / 20631  0.5537782947990888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-XyaxXPr__N",
        "colab_type": "text"
      },
      "source": [
        "# combined 2 ML approaches + graphing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqFkkp0Frwfl",
        "colab_type": "code",
        "outputId": "2778198e-afc3-436b-cb40-ee4ee6681257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#method 2\n",
        "class SentimentClassifier1:\n",
        "    def train(self, trainX, trainY):\n",
        "        self.clf = DummyClassifier(strategy='most_frequent')\n",
        "        self.clf.fit(trainX, trainY)\n",
        "\n",
        "    def classify(self, testX):\n",
        "        # same as return ['EOS' for i in range(len(testX))]\n",
        "        return self.clf.predict(testX)\n",
        "\n",
        "class SentimentClassifier2:\n",
        "    def train(self, trainX, trainY):\n",
        "        self.clf = RandomForestClassifier()\n",
        "        self.clf.fit(trainX, trainY)\n",
        "\n",
        "    def classify(self, testX):\n",
        "        return self.clf.predict(testX)\n",
        "\n",
        "class SentimentClassifier3:\n",
        "    def train(self, trainX, trainY):\n",
        "        self.clf = DecisionTreeClassifier(max_depth=4, max_leaf_nodes=3)\n",
        "        self.clf.fit(trainX, trainY)\n",
        "\n",
        "    def classify(self, testX):\n",
        "        return self.clf.predict(testX)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file) as fin:\n",
        "        X = []\n",
        "        y = []\n",
        "        for line in fin:\n",
        "            #print(line)\n",
        "            arr = line.strip().split('\\t', 1)\n",
        "            X.append(arr[1])\n",
        "            y.append(arr[0])\n",
        "        return X, y\n",
        "\n",
        "def get_sentiments(file):\n",
        "    sentiments = {}\n",
        "    arr = []\n",
        "    with open(file) as fin:\n",
        "         for line in fin:\n",
        "             arr.append(line.split())\n",
        "    for line in arr:\n",
        "        word = line[2][6:]\n",
        "        senti = line[5][14:]\n",
        "        sentiments[word] = senti\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "def addFeatures(xFile):\n",
        "    newFeatures = []\n",
        "    dataset = get_sentiments(\"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/sentiments.txt\")\n",
        "    for i in range(len(xFile)):\n",
        "        newFeatures.append([0,0,0])\n",
        "        tweet = xFile[i]\n",
        "        for word in tweet:\n",
        "            if word.find('!') != -1:\n",
        "                newFeatures[i][0] = 1\n",
        "            if ':)' in word or ';)' in word:\n",
        "                newFeatures[i][1] = 1\n",
        "            elif ':(' in word or ';(' in word:\n",
        "                newFeatures[i][1] = -1\n",
        "            if word in dataset:\n",
        "                if dataset[word] == 'positive':\n",
        "                    newFeatures[i][2] = newFeatures[i][2] + 1\n",
        "                else:\n",
        "                    newFeatures[i][2] = newFeatures[i][2] - 1\n",
        "\n",
        "    return newFeatures\n",
        "\n",
        "def evaluate(outputs, golds):\n",
        "    correct = 0\n",
        "    for h, y in zip(outputs, golds):\n",
        "        if h == y:\n",
        "            correct += 1\n",
        "    print(f'{correct} / {len(golds)}  {correct / len(golds)}')\n",
        "    result = correct / len(golds)\n",
        "    return result\n",
        "    #print(f'{correct / len(golds)}')\n",
        "\n",
        "#method 1\n",
        "with open(\"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2016_raw.txt\") as fin:\n",
        "    tweets = []\n",
        "    senses = []\n",
        "    sentiments = []\n",
        "    for line in fin:\n",
        "        arr = line.lower().strip().split()\n",
        "        tweets.append(line)\n",
        "        sentiments.append(arr[0])\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        if sentiments[i] == \"positive\":\n",
        "            senses.append('positive')\n",
        "        elif sentiments[i] == \"negative\":\n",
        "            senses.append('negative')\n",
        "        else:\n",
        "            senses.append('neutral')\n",
        "def get_results(X, y, testX, testY):\n",
        "    dummy = SentimentClassifier1()\n",
        "    dummy.train(X, y)\n",
        "    outputsDummy = dummy.classify(testX)\n",
        "\n",
        "    newFeatures = addFeatures(X)\n",
        "    newTestFeatures = addFeatures(testX)\n",
        "    randomForest = SentimentClassifier2()\n",
        "    decisionTree = SentimentClassifier3()\n",
        "\n",
        "    randomForest.train(newFeatures, y)\n",
        "    outputsRF = randomForest.classify(newTestFeatures)\n",
        "\n",
        "    decisionTree.train(newFeatures, y)\n",
        "    outputsDecision = decisionTree.classify(newTestFeatures)\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(len(outputsDummy)):\n",
        "        final = []\n",
        "        final.append(outputsDummy[i])\n",
        "        final.append(outputsRF[i])\n",
        "        final.append(outputsDecision[i])\n",
        "        outputs.append(max(set(final), key=final.count))\n",
        "    return evaluate(outputs, testY)\n",
        "\n",
        "def features():\n",
        "    vector = TfidfVectorizer()\n",
        "    vector.fit(tweets)\n",
        "    feats = (vector.transform(tweets))\n",
        "    trainX, testX, trainY, testY = train_test_split(feats, senses, train_size = 0.5, test_size = 0.5)\n",
        "    return trainX, testX, trainY, testY\n",
        "\n",
        "def classifier(trainX, testX, trainY, testY):\n",
        "    model = ExtraTreeClassifier()\n",
        "    model = model.fit(trainX, trainY)\n",
        "    prediction = model.predict(testX)\n",
        "    return prediction\n",
        "\n",
        "def main():\n",
        "\n",
        "    # for method 1\n",
        "    trainX, testX, trainY, testY = features()\n",
        "    prediction = classifier(trainX, testX, trainY, testY)\n",
        "    accuracy = accuracy_score(testY, prediction)\n",
        "    print(\"Method #1:\")\n",
        "    print(accuracy)\n",
        "\n",
        "    # for method 2\n",
        "    \n",
        "    print(\"Method #2:\")\n",
        "    X, y = load_data(\"/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/semeval_train.txt\")\n",
        "\n",
        "    testX1, testY1 = load_data('/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2013_raw.txt')\n",
        "    testX2, testY2 = load_data('/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2014_raw.txt')\n",
        "    testX3, testY3 = load_data('/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2015_raw.txt')\n",
        "    testX4, testY4 = load_data('/content/drive/My Drive/Information Retrieval Project/twitter_sentiment/Twitter2016_raw.txt')\n",
        "\n",
        "    print(\"2013:\")\n",
        "    percent1 = get_results(X, y, testX1, testY1)\n",
        "    print(\"2014:\")\n",
        "    percent2 = get_results(X, y, testX2, testY2)\n",
        "    print(\"2015:\")\n",
        "    percent3 = get_results(X, y, testX3, testY3)\n",
        "    print(\"2016:\")\n",
        "    percent4 = get_results(X, y, testX4, testY4)\n",
        "\n",
        "    avg = (percent1 + percent2 + percent3 + percent4) / 4\n",
        "    print(\"avg: \", avg)\n",
        "\n",
        "\n",
        "    # graph comparison\n",
        "    objects = ('Method #1', 'Method #2')\n",
        "    y_pos = np.arange(len(objects))\n",
        "    performance = [accuracy, avg]\n",
        "\n",
        "    plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
        "    plt.xticks(y_pos, objects)\n",
        "    plt.ylabel('Percentage Accuracy')\n",
        "    plt.title('Comparison of 2 ML Classifiers')\n",
        "\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Method #1:\n",
            "0.6766188445133773\n",
            "Method #2:\n",
            "2013:\n",
            "1960 / 3812  0.5141657922350472\n",
            "2014:\n",
            "983 / 1852  0.5307775377969762\n",
            "2015:\n",
            "1200 / 2389  0.5023022185014651\n",
            "2016:\n",
            "11425 / 20631  0.5537782947990888\n",
            "avg:  0.5252559608331444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}